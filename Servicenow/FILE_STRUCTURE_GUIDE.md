# SmartOps - File-by-File Breakdown

## PROJECT STRUCTURE OVERVIEW

```
smartops-dev/
â”œâ”€â”€ ROOT FILES (Data & Config)
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ .gitignore
â”‚   â”œâ”€â”€ LLM_USAGE_SUMMARY.md
â”‚   â”œâ”€â”€ WORKFLOW_DIAGRAMS.md
â”‚   â””â”€â”€ DATA FILES (CSV & Excel)
â”‚       â”œâ”€â”€ incidents_store_pharma_v3.csv
â”‚       â”œâ”€â”€ kb_store_pharma_v3.csv
â”‚       â”œâ”€â”€ problems_store_pharma_v3.csv
â”‚       â”œâ”€â”€ sample_test.csv
â”‚       â””â”€â”€ *.xlsx (output files)
â”‚
â”œâ”€â”€ servicenow_api_automation/
â”‚   â”œâ”€â”€ MAIN ENTRY POINT
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”‚
â”‚   â”œâ”€â”€ CORE MODULES
â”‚   â”‚   â”œâ”€â”€ servicenow_client.py (API communication)
â”‚   â”‚   â”œâ”€â”€ workflow.py (orchestration)
â”‚   â”‚   â”œâ”€â”€ ticket_segregation_engine.py (TF-IDF matching)
â”‚   â”‚   â””â”€â”€ task_assignment.py (team assignment)
â”‚   â”‚
â”‚   â”œâ”€â”€ MOCK & TEST DATA
â”‚   â”‚   â”œâ”€â”€ mock_servicenow_data.py (generates fake data)
â”‚   â”‚   â”œâ”€â”€ mock_tickets.json
â”‚   â”‚   â””â”€â”€ mock_categories.json
â”‚   â”‚
â”‚   â”œâ”€â”€ CONFIGURATION
â”‚   â”‚   â”œâ”€â”€ config.example.py
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ DOCUMENTATION
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ FLOW_EXPLANATION.md
â”‚   â”‚   â””â”€â”€ DELIVERY_SUMMARY.md
â”‚   â”‚
â”‚   â”œâ”€â”€ UI INTERFACE
â”‚   â”‚   â””â”€â”€ streamlit_app.py
â”‚   â”‚
â”‚   â””â”€â”€ TESTS
â”‚       â””â”€â”€ tests/
â”‚           â”œâ”€â”€ test_servicenow_client.py
â”‚           â”œâ”€â”€ test_assignment.py
â”‚           â””â”€â”€ test_segregation.py
â”‚
â””â”€â”€ ticket_segregation_workflow/
    â”œâ”€â”€ MAIN ENTRY POINT
    â”‚   â””â”€â”€ ticket_segregation.py (standalone segregation)
    â”‚
    â”œâ”€â”€ UTILITIES
    â”‚   â”œâ”€â”€ workflow_utils.py (helper functions)
    â”‚   â”œâ”€â”€ apply_manual_labels.py (label management)
    â”‚   â””â”€â”€ generate_sample_data.py (test data generation)
    â”‚
    â”œâ”€â”€ CONFIGURATION
    â”‚   â”œâ”€â”€ .env.example
    â”‚   â””â”€â”€ requirements.txt
    â”‚
    â”œâ”€â”€ DOCUMENTATION
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ ARCHITECTURE.md
    â”‚   â”œâ”€â”€ PROJECT_SUMMARY.md
    â”‚   â”œâ”€â”€ QUICKSTART.md
    â”‚   â””â”€â”€ DELIVERY_SUMMARY.txt
    â”‚
    â”œâ”€â”€ DOCKER
    â”‚   â””â”€â”€ Dockerfile
    â”‚
    â”œâ”€â”€ DATA FILES
    â”‚   â”œâ”€â”€ ticket_categories.xlsx
    â”‚   â”œâ”€â”€ segregated_tickets_output.xlsx
    â”‚   â””â”€â”€ servicenow_tickets.xlsx
    â”‚
    â”œâ”€â”€ DEPLOYMENT
    â”‚   â”œâ”€â”€ git_push.ps1
    â”‚   â””â”€â”€ .github/ (CI/CD workflows)
    â”‚
    â”œâ”€â”€ UI INTERFACE
    â”‚   â””â”€â”€ streamlit_app.py
    â”‚
    â””â”€â”€ TESTS
        â””â”€â”€ tests/
            â”œâ”€â”€ test_matching.py
            â””â”€â”€ test_workflow_utils.py

â”œâ”€â”€ NOTEBOOKS (Exploratory & Production)
â”‚   â”œâ”€â”€ pharmacy_phi_redaction_production.ipynb (PII masking)
â”‚   â”œâ”€â”€ pharma_data_processing scrubbed.ipynb (data prep)
â”‚   â””â”€â”€ pii_masking.ipynb (legacy)

â””â”€â”€ EMBEDDINGS & PICKLES
    â”œâ”€â”€ KA_embeddings.pkl (knowledge article embeddings)
    â””â”€â”€ Problem_embeddings.pkl (problem embeddings)
```

---

## 1. ROOT LEVEL FILES

### README.md
**Purpose:** Project overview and quick start guide
- High-level description of SmartOps
- Key features & benefits
- Quick start instructions
- Contact information

### requirements.txt
**Purpose:** Project-wide Python dependencies
```
pandas
scikit-learn
python-dotenv
openai
requests
openpyxl
streamlit
spacy
presidio-analyzer
presidio-anonymizer
transformers
torch
```

### .gitignore
**Purpose:** Git exclusions
- `__pycache__/`
- `.env` (secrets)
- `*.xlsx` (large outputs)
- `.venv/` (virtual environment)
- `*.pkl` (embeddings)

### LLM_USAGE_SUMMARY.md
**Purpose:** Comprehensive LLM and AI model documentation
- TF-IDF vectorization details
- OpenAI GPT-3.5 integration guide
- spaCy NER setup
- Presidio framework
- HuggingFace biomedical NER
- Cost analysis & recommendations

### WORKFLOW_DIAGRAMS.md
**Purpose:** ASCII diagrams for all 7 phases
- Main workflow overview
- Phase-by-phase flows
- Decision trees
- Error handling
- Execution modes

---

## 2. SERVICENOW API AUTOMATION PROJECT

### 2.1 MAIN ENTRY POINT

#### `main.py` (194 lines)
**Purpose:** Primary orchestration script for the entire workflow
**Key Functions:**
- `load_config()` â†’ Loads credentials from config.py or .env
- `run_with_api()` â†’ Executes with real ServiceNow API
- `run_with_mock()` â†’ Executes with generated mock data
- `run_with_dry_run()` â†’ Simulates without updates

**Usage:**
```bash
python main.py                      # Live mode (updates ServiceNow)
python main.py --dry-run            # Simulation mode
python main.py --mock               # Test mode (no API calls)
python main.py --tickets 200        # Process 200 tickets
```

**Workflow:**
```
main.py
    â”‚
    â”œâ”€ Load config & environment variables
    â”œâ”€ Parse command-line arguments (--mock, --dry-run)
    â”œâ”€ Initialize ServiceNowClient
    â”œâ”€ Initialize TicketSegregationWorkflow
    â”œâ”€ Run workflow (fetch â†’ segregate â†’ assign â†’ update)
    â”œâ”€ Export results to Excel
    â””â”€ Print summary & statistics
```

---

### 2.2 CORE MODULES

#### `servicenow_client.py` (275 lines)
**Purpose:** API client for all ServiceNow REST API calls
**Key Classes:**
```python
class ServiceNowClient:
    - __init__(instance_url, username, password)
    - test_connection()          # Verify API access
    - fetch_tickets(limit, query)     # GET /api/now/table/incident
    - fetch_categories(limit)         # GET /api/now/table/incident_category
    - assign_ticket(number, assigned_to, assignment_group)  # PATCH
    - create_change_log_entry()       # POST audit trail
```

**Key Features:**
- âœ… Authentication (basic auth + session)
- âœ… Pagination (sysparm_offset, sysparm_limit)
- âœ… Rate limiting (backoff & retry)
- âœ… Error handling (retries on timeout)
- âœ… Logging (audit trail)

**Example:**
```python
from servicenow_client import ServiceNowClient

client = ServiceNowClient(
    instance_url="https://dev12345.service-now.com",
    username="your_user",
    password="your_pass"
)

# Test connection
if client.test_connection():
    print("âœ“ Connected")

# Fetch tickets
tickets = client.fetch_tickets(limit=100, query={'state': '1'})

# Assign ticket
client.assign_ticket(
    ticket_number='INC0010001',
    assigned_to='user@company.com',
    assignment_group='DBA_Group'
)
```

---

#### `workflow.py` (TBD - Orchestration)
**Purpose:** Main orchestrator that ties all modules together
**Pseudo-Structure:**
```python
class TicketSegregationWorkflow:
    def __init__(servicenow_config, team_mapping, confidence_threshold)
    def run(ticket_limit, category_limit, dry_run)
        â”œâ”€ fetch_tickets() â†’ servicenow_client.fetch_tickets()
        â”œâ”€ fetch_categories() â†’ servicenow_client.fetch_categories()
        â”œâ”€ segregate() â†’ TicketSegregationEngine.segregate_tickets()
        â”œâ”€ assign_teams() â†’ TaskAssignmentEngine.bulk_assign()
        â”œâ”€ update_servicenow() â†’ servicenow_client.assign_ticket() [if not dry_run]
        â””â”€ export_results() â†’ pd.ExcelWriter()
```

---

#### `ticket_segregation_engine.py` (TBD - TF-IDF Matching)
**Purpose:** Core AI/ML matching engine using TF-IDF
**Pseudo-Structure:**
```python
class TicketSegregationEngine:
    def __init__(tickets_df, categories_df)
    
    def match_ticket_to_category(ticket_description)
        â”œâ”€ TfidfVectorizer.fit_transform() (100 features, bigrams)
        â”œâ”€ cosine_similarity() (ticket vs all categories)
        â””â”€ return (best_category, confidence_score)
    
    def segregate_tickets(confidence_threshold=0.3)
        â””â”€ Loop 100 tickets â†’ match â†’ store in segregated_data[category]
    
    def validate_with_llm(use_llm=True)  [Optional]
        â””â”€ For each unknown ticket â†’ Call GPT-3.5 â†’ Suggest category
    
    def save_to_excel(output_file)
        â””â”€ Create sheets per category + Unknown sheet
```

**TF-IDF Algorithm:**
```
Input: Ticket description ("Database down")
       + 15 category descriptions

Step 1: Vectorization
  â””â”€ TfidfVectorizer converts all text to 100D vectors

Step 2: Similarity
  â””â”€ cosine_similarity(ticket_vector, category_vectors) â†’ [0.05, 0.85, 0.12, ...]

Step 3: Decision
  â””â”€ Max confidence = 0.85
  â””â”€ 0.85 >= 0.3 threshold? YES â†’ MATCHED to best_category
  â””â”€ Store in segregated_data["DB_Admin"]
```

---

#### `task_assignment.py` (TBD - Team Mapping)
**Purpose:** Assign tickets to teams based on TEAM_MAPPING
**Pseudo-Structure:**
```python
class TaskAssignmentEngine:
    def __init__(team_mapping)
    
    def bulk_assign(segregation_results)
        â””â”€ For each matched ticket:
           â”œâ”€ Lookup category in TEAM_MAPPING
           â”œâ”€ Extract: assigned_to, assignment_group, priority
           â””â”€ Attach to ticket record
    
    def validate_mapping()
        â””â”€ Check all categories have team mappings
```

**TEAM_MAPPING Example:**
```python
TEAM_MAPPING = {
    "DB Admin": {
        "team": "DBA Team",
        "assigned_to": "dba_lead@company.com",
        "assignment_group": "DBA_Group",
        "priority": "critical"
    },
    "Network": {
        "team": "Network Team",
        "assigned_to": "network@company.com",
        "assignment_group": "NET_Group",
        "priority": "high"
    },
    # ... 13 more categories
}
```

---

### 2.3 MOCK & TEST DATA

#### `mock_servicenow_data.py` (Generator)
**Purpose:** Generates fake ServiceNow data for testing
**Key Class:**
```python
class MockServiceNowGenerator:
    def generate_tickets(count=80)      # Create fake tickets
    def generate_categories(count=15)   # Create fake categories
```

**Usage:**
```python
from mock_servicenow_data import MockServiceNowGenerator

gen = MockServiceNowGenerator()
fake_tickets = gen.generate_tickets(100)
fake_categories = gen.generate_categories(15)
```

#### `mock_tickets.json`
**Purpose:** Sample ticket data in JSON format
**Structure:**
```json
[
  {
    "number": "INC0010001",
    "short_description": "Database down",
    "description": "Production database unreachable for 30 minutes",
    "state": "1",
    "created": "2025-12-02T10:00:00Z"
  },
  ...
]
```

#### `mock_categories.json`
**Purpose:** Sample category data
**Structure:**
```json
[
  {
    "sys_id": "cat_001",
    "name": "Database Backup",
    "description": "Database backup and restore issues"
  },
  ...
]
```

---

### 2.4 CONFIGURATION

#### `config.example.py`
**Purpose:** Template for configuration (copy to `config.py`)
**Contents:**
```python
# ServiceNow credentials
SERVICENOW_INSTANCE_URL = "https://dev12345.service-now.com"
SERVICENOW_USERNAME = "your_username"
SERVICENOW_PASSWORD = "your_password"

# API limits
TICKET_LIMIT = 100
CATEGORY_LIMIT = 100

# Confidence threshold for TF-IDF matching
CONFIDENCE_THRESHOLD = 0.3

# Team mapping (categories â†’ team assignments)
TEAM_MAPPING = {
    "DB Admin": {
        "team": "DBA Team",
        "assigned_to": "dba_lead@company.com",
        "assignment_group": "DBA_Group",
        "priority": "critical"
    },
    # ... 14 more mappings
}
```

---

### 2.5 DOCUMENTATION

#### `README.md` (Project-specific)
**Purpose:** ServiceNow API Automation project guide
- Quick start: `python main.py`
- Configuration steps
- API flow explanation
- Troubleshooting

#### `FLOW_EXPLANATION.md` (Detailed diagrams)
**Purpose:** Complete step-by-step flow with visuals
- High-level workflow
- Detailed phase flows
- API request/response examples
- Data transformation pipeline

#### `DELIVERY_SUMMARY.md`
**Purpose:** Project delivery notes & status
- Completed features
- Known limitations
- Next steps
- Performance metrics

---

### 2.6 UI INTERFACE

#### `streamlit_app.py`
**Purpose:** Web dashboard for ticket segregation
**Features:**
```python
# UI Components
- Input: Upload Excel or connect to ServiceNow
- Configuration: Set threshold, team mapping
- Execution: Run segregation
- Output: Display results, download Excel
- Monitoring: Show progress, stats, errors
```

**Usage:**
```bash
streamlit run streamlit_app.py
# Opens: http://localhost:8501
```

---

### 2.7 TESTS

#### `tests/test_servicenow_client.py`
**Purpose:** Unit tests for ServiceNow API client
**Test Cases:**
```python
def test_connection()           # Verify API connectivity
def test_fetch_tickets()        # Mock API responses
def test_fetch_categories()     # Category fetching
def test_rate_limiting()        # Rate limit handling
def test_error_handling()       # Error recovery
```

#### `tests/test_assignment.py`
**Purpose:** Tests for team assignment logic
```python
def test_team_mapping()         # Verify mappings
def test_bulk_assign()          # Assign multiple tickets
def test_missing_mapping()      # Handle missing teams
```

#### `tests/test_segregation.py`
**Purpose:** Tests for TF-IDF matching
```python
def test_tfidf_matching()       # Confidence scoring
def test_unknown_tickets()      # Below-threshold handling
def test_llm_validation()       # GPT-3.5 integration (mock)
```

---

## 3. TICKET SEGREGATION WORKFLOW PROJECT

### 3.1 MAIN ENTRY POINT

#### `ticket_segregation.py` (310 lines - Standalone)
**Purpose:** Standalone ticket segregation without ServiceNow API
**Key Class:**
```python
class TicketSegregationEngine:
    def __init__(tickets_file, categories_file, tickets_df, categories_df)
    def match_ticket_to_category(ticket_description)
    def segregate_tickets(confidence_threshold=0.3)
    def validate_with_llm(use_llm=False)        # Optional OpenAI
    def save_to_excel(output_file)
    def generate_segregation_report()
```

**Usage:**
```python
from ticket_segregation import TicketSegregationEngine

# From files
engine = TicketSegregationEngine(
    tickets_file='servicenow_tickets.xlsx',
    categories_file='ticket_categories.xlsx'
)

# Or from DataFrames (from API)
engine = TicketSegregationEngine(
    tickets_df=tickets_df,
    categories_df=categories_df
)

# Segregate
engine.segregate_tickets(confidence_threshold=0.3)

# Optional: Validate with GPT-3.5
engine.validate_with_llm(use_llm=True)

# Export
engine.save_to_excel('segregated_tickets_output.xlsx')
```

**Algorithm Details:**
```
For each ticket:
  1. Extract keywords from description
  2. TF-IDF vectorize vs 15 category descriptions
  3. Calculate cosine similarity scores
  4. Find best match (highest score)
  5. Compare vs threshold (0.3):
     â”œâ”€ score >= 0.3 â†’ MATCHED (store in category sheet)
     â””â”€ score < 0.3  â†’ UNKNOWN (optional LLM validation)
  6. Output: segregated_data[category_name] = [ticket1, ticket2, ...]

Save to Excel:
  â”œâ”€ Sheet: Summary (stats, charts)
  â”œâ”€ Sheet: DB Admin (15 matched tickets)
  â”œâ”€ Sheet: Network (12 matched tickets)
  â”œâ”€ ... (13 more category sheets)
  â””â”€ Sheet: Unknown_101 (25 unmatched tickets)
```

---

### 3.2 UTILITIES

#### `workflow_utils.py` (Helper functions)
**Purpose:** Common utility functions
```python
def load_excel_file(path)          # Read Excel
def save_excel_file(df, path)      # Write Excel
def extract_keywords(text)         # Text preprocessing
def validate_dataframe(df)         # Data validation
def generate_report(results)       # Report generation
```

#### `apply_manual_labels.py` (Label management)
**Purpose:** Manually label tickets for training data
**Features:**
- Interactive CLI for labeling
- Save labels to Excel
- Review labeled data
- Generate training dataset

**Usage:**
```bash
python apply_manual_labels.py
# Interactive: "Enter ticket number, category, confidence"
```

#### `generate_sample_data.py` (Test data)
**Purpose:** Generate sample tickets & categories for testing
```python
def generate_sample_tickets(count=80)
def generate_sample_categories(count=15)
```

---

### 3.3 CONFIGURATION

#### `.env.example`
**Purpose:** Template for environment variables
```
SERVICENOW_INSTANCE_URL=https://dev12345.service-now.com
SERVICENOW_USERNAME=your_username
SERVICENOW_PASSWORD=your_password
OPENAI_API_KEY=sk-xxx...xxx
CONFIDENCE_THRESHOLD=0.3
TICKET_LIMIT=100
```

#### `requirements.txt`
**Purpose:** Project dependencies
```
pandas
scikit-learn
openpyxl
python-dotenv
openai
spacy
```

---

### 3.4 DOCUMENTATION

#### `README.md`
**Purpose:** Quick start & usage
```markdown
# Ticket Segregation Workflow

## Quick Start
```bash
python ticket_segregation.py
```

## Features
- TF-IDF matching
- Optional LLM validation
- Excel export
- Category breakdown
```

#### `ARCHITECTURE.md`
**Purpose:** System design & architecture
- Component diagram
- Data flow
- Algorithm details
- Performance metrics

#### `PROJECT_SUMMARY.md`
**Purpose:** High-level project overview
- Goals & objectives
- Deliverables
- Status
- Metrics

#### `QUICKSTART.md`
**Purpose:** 5-minute setup guide
```
1. Install: pip install -r requirements.txt
2. Copy .env.example to .env
3. Run: python ticket_segregation.py
4. Output: segregated_tickets_output.xlsx
```

#### `DELIVERY_SUMMARY.txt`
**Purpose:** Release notes & deliverables
- What was built
- Features completed
- Known issues
- Next phase

---

### 3.5 DOCKER

#### `Dockerfile`
**Purpose:** Containerization for deployment
```dockerfile
FROM python:3.11
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "ticket_segregation.py"]
```

**Usage:**
```bash
docker build -t smartops-seg .
docker run smartops-seg
```

---

### 3.6 DATA FILES

#### `ticket_categories.xlsx`
**Purpose:** Known categories (master data)
**Columns:**
```
Category_ID | Category_Name | Description
    cat_001 | DB Admin      | Database administration issues
    cat_002 | Network       | Network connectivity issues
    ...
```

#### `segregated_tickets_output.xlsx`
**Purpose:** Output file from segregation
**Sheets:**
```
Summary          â†’ Statistics & charts
DB Admin (15)    â†’ Matched tickets
Network (12)     â†’ Matched tickets
...
Unknown_101 (25) â†’ Unmatched tickets (need manual review)
```

#### `servicenow_tickets.xlsx`
**Purpose:** Sample input file
**Columns:**
```
Ticket_Number | Short_Description | Description | Created
   INC0010001 | Database down     | Production DB...
   INC0010002 | VPN issue         | Cannot connect...
```

---

### 3.7 DEPLOYMENT

#### `git_push.ps1` (PowerShell script)
**Purpose:** Automated Git commit & push
```powershell
# Stages changes, commits with message, pushes to remote
.\git_push.ps1
```

#### `.github/` (CI/CD workflows)
**Purpose:** GitHub Actions automation
- Auto-test on push
- Auto-deploy to staging
- Auto-release to production

---

### 3.8 TESTS

#### `tests/test_matching.py`
**Purpose:** Unit tests for TF-IDF matching
```python
def test_exact_match()         # Perfect match scoring
def test_partial_match()       # Partial match handling
def test_no_match()            # Below-threshold handling
def test_tfidf_vectorization() # Vector accuracy
```

#### `tests/test_workflow_utils.py`
**Purpose:** Tests for utility functions
```python
def test_load_excel()          # File I/O
def test_extract_keywords()    # Text preprocessing
def test_generate_report()     # Report generation
```

---

## 4. JUPYTER NOTEBOOKS (Exploratory)

### `pharmacy_phi_redaction_production.ipynb` (Production-ready)
**Purpose:** PHI/PII redaction for pharmacy records
**Key Components:**
1. **Presidio Framework** - PII detection
2. **spaCy NER** - Named entity recognition (11MB small model)
3. **HuggingFace Biomedical NER** - Disease detection (107 entity types)
4. **Custom Recognizers** - Rx#, Fill ID, MRN patterns
5. **Anonymization Engine** - Redact sensitive data

**Performance:**
- ~200ms per record
- 1000+ records per hour
- HIPAA-compliant

### `pharma_data_processing scrubbed.ipynb`
**Purpose:** Data preparation & cleaning
- Load CSV files
- Clean & normalize
- Handle missing values
- Export for downstream processing

### `pii_masking.ipynb` (Legacy)
**Purpose:** Earlier version of PHI redaction
- Kept for reference
- Superseded by `pharmacy_phi_redaction_production.ipynb`

---

## 5. EMBEDDINGS & PICKLES

### `KA_embeddings.pkl`
**Purpose:** Pre-computed embeddings for knowledge articles
**Format:** Pickled NumPy arrays
**Size:** ~5MB
**Usage:** Knowledge article similarity matching

### `Problem_embeddings.pkl`
**Purpose:** Pre-computed embeddings for problems
**Format:** Pickled NumPy arrays
**Size:** ~3MB
**Usage:** Problem pattern matching

---

## QUICK REFERENCE: FILE DEPENDENCIES

```
main.py (servicenow_api_automation)
  â”œâ”€ imports servicenow_client.py
  â”œâ”€ imports workflow.py
  â”œâ”€ imports task_assignment.py
  â”œâ”€ imports config.py
  â””â”€ imports mock_servicenow_data.py

workflow.py
  â”œâ”€ imports servicenow_client.py
  â”œâ”€ imports ticket_segregation_engine.py
  â””â”€ imports task_assignment.py

ticket_segregation_engine.py
  â”œâ”€ imports sklearn (TfidfVectorizer, cosine_similarity)
  â”œâ”€ imports pandas (DataFrame operations)
  â””â”€ imports openai (optional, for LLM validation)

ticket_segregation.py (ticket_segregation_workflow)
  â”œâ”€ imports sklearn
  â”œâ”€ imports pandas
  â”œâ”€ imports workflow_utils.py
  â””â”€ imports openai (optional)

streamlit_app.py
  â”œâ”€ imports servicenow_client.py
  â”œâ”€ imports workflow.py
  â””â”€ imports pandas & streamlit
```

---

## EXECUTION FLOWS

### Flow 1: ServiceNow API Automation (Recommended)
```
python servicenow_api_automation/main.py
    â”‚
    â”œâ”€ Load config.py & .env
    â”œâ”€ Create ServiceNowClient
    â”œâ”€ Create TicketSegregationWorkflow
    â”œâ”€ Fetch tickets & categories via API
    â”œâ”€ Segregate using TF-IDF
    â”œâ”€ Assign teams using TEAM_MAPPING
    â”œâ”€ Update ServiceNow (or dry-run)
    â””â”€ Export Excel + print summary
```

### Flow 2: Standalone Segregation (Files Only)
```
python ticket_segregation_workflow/ticket_segregation.py
    â”‚
    â”œâ”€ Load tickets.xlsx
    â”œâ”€ Load categories.xlsx
    â”œâ”€ Segregate using TF-IDF
    â”œâ”€ Optional: LLM validation
    â””â”€ Export Excel
```

### Flow 3: Web Dashboard
```
streamlit run servicenow_api_automation/streamlit_app.py
    â”‚
    â”œâ”€ Upload files or API config
    â”œâ”€ Configure parameters
    â”œâ”€ Execute workflow
    â””â”€ Download results
```

---

## SUMMARY TABLE

| Component | Files | Purpose | Technology |
|-----------|-------|---------|-----------|
| **API Client** | servicenow_client.py | ServiceNow REST API | requests, auth |
| **Matching Engine** | ticket_segregation_engine.py | TF-IDF + cosine sim | scikit-learn |
| **Workflow** | workflow.py, main.py | Orchestration | Python async |
| **Team Assignment** | task_assignment.py | Route to teams | config TEAM_MAPPING |
| **Optional LLM** | (integrated) | GPT-3.5 validation | OpenAI API |
| **Excel Export** | (all save methods) | Output format | openpyxl |
| **Web UI** | streamlit_app.py | Dashboard | streamlit |
| **PII Redaction** | pharmacy_*.ipynb | Data masking | Presidio, spaCy |
| **Testing** | tests/*.py | Validation | pytest |

---

## KEY TAKEAWAYS

1. **Two entry points:**
   - `servicenow_api_automation/main.py` â†’ Real-time with API
   - `ticket_segregation_workflow/ticket_segregation.py` â†’ File-based standalone

2. **Core algorithm:** TF-IDF + Cosine Similarity (fast, free, no API)

3. **Optional enhancement:** GPT-3.5 for unknown tickets validation

4. **Output:** Excel with per-category sheets + statistics

5. **Safe testing:** `--dry-run` mode shows changes without updating ServiceNow

6. **Bonus:** PHI redaction notebooks for sensitive data handling

7. **Configuration:** Copy `config.example.py` and `.env.example` to customize

---

This file-wise breakdown shows how all components work together to automate ticket segregation and team assignment! ðŸš€
